# Hiring Assistant Service

> Source: https://deepwiki.com/harleenkaur28/AI-Resume-Parser/3.4-hiring-assistant-service

# Hiring Assistant Service

Relevant source files

* [backend/app/agents/\_\_init\_\_.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py)
* [backend/app/core/\_\_init\_\_.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/__init__.py)
* [backend/app/core/llm.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py)
* [backend/app/models/schemas.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py)
* [backend/app/services/ats.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/ats.py)
* [backend/app/services/cold\_mail.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py)
* [backend/app/services/linkedin\_profile.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/linkedin_profile.py)
* [backend/app/services/tailored\_resume.py](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/tailored_resume.py)

## Purpose and Scope

The Hiring Assistant Service generates role-specific answers to interview questions, leveraging resume context and company research to provide personalized, relevant responses. This service helps job seekers prepare for interviews by analyzing their background against the target role and company, producing tailored answers within specified word limits.

This document covers the interview answer generation service. For resume analysis capabilities, see [Resume Analysis Service](/harleenkaur28/AI-Resume-Parser/3.1-resume-analysis-service). For cold email generation that also uses company research, see [Cold Mail Generator Service](/harleenkaur28/AI-Resume-Parser/3.3-cold-mail-generator-service). For the web search and content extraction agents used by this service, see [Support Agents](/harleenkaur28/AI-Resume-Parser/3.7-support-agents).

**Sources:** [backend/app/models/schemas.py204-238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L238) Diagram 2 (Backend AI/ML Service Architecture)

---

## Service Architecture Overview

The Hiring Assistant Service follows a three-stage pipeline: request validation, company research enrichment, and LLM-powered answer generation. The service integrates with external research agents to gather company context and uses Google Gemini for natural language generation.

**Sources:** [backend/app/services/cold\_mail.py7](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L7-L7) [backend/app/core/llm.py1-37](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L1-L37) [backend/app/agents/\_\_init\_\_.py1-13](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py#L1-L13) Diagram 2 (Backend AI/ML Service Architecture)

---

## Request and Response Models

### HiringAssistantRequest

The service accepts structured interview preparation requests with role context, questions, and company information.

| Field | Type | Required | Default | Description |
| --- | --- | --- | --- | --- |
| `role` | `str` | Yes | - | Target role (e.g., "Software Engineer") |
| `questions` | `List[str]` | No | `[]` | List of interview questions to answer |
| `company_name` | `str` | Yes | - | Target company name |
| `user_knowledge` | `Optional[str]` | No | `""` | Candidate's existing knowledge about company/role |
| `company_url` | `Optional[str]` | No | `None` | Company website URL for research |
| `word_limit` | `Optional[int]` | No | `150` | Word limit per answer (50-500 range) |

**Validation Rules:**

* `role`: Minimum length 1 character [backend/app/models/schemas.py205-208](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L205-L208)
* `company_name`: Minimum length 1 character [backend/app/models/schemas.py214-217](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L214-L217)
* `word_limit`: Range validated between 50 and 500 words [backend/app/models/schemas.py227-232](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L227-L232)

### HiringAssistantResponse

The service returns a structured response mapping each question to its generated answer.

![Architecture Diagram](images/3.4-hiring-assistant-service_diagram_1.png)
```

**Response Structure:**

* `success`: Boolean indicating operation success (default: `True`) [backend/app/models/schemas.py236](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L236-L236)
* `message`: Human-readable status message (default: "Answers generated successfully.") [backend/app/models/schemas.py237](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L237-L237)
* `data`: Dictionary mapping questions to answers [backend/app/models/schemas.py238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L238-L238)

**Sources:** [backend/app/models/schemas.py204-238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L238)

---

## Company Research Integration

The Hiring Assistant Service enriches answer generation with company-specific context through the `get_company_research()` function. This function is imported from `app.services.hiring_assiatnat` and provides contextual information about the target company.

### Research Flow

```
![Architecture Diagram](images/3.4-hiring-assistant-service_diagram_2.png)
```

### Research Usage Pattern

The service conditionally fetches company research based on the presence of `company_url`:

```
```
# Pattern from cold_mail.py that hiring assistant follows
company_research_info = ""

if company_url:
    company_research_info = get_company_research(company_name, company_url)
```
```

This pattern is consistent across services that require company context, including the Cold Mail Generator [backend/app/services/cold\_mail.py308-311](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L308-L311)

**Sources:** [backend/app/services/cold\_mail.py7-311](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L7-L311) [backend/app/agents/\_\_init\_\_.py7](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py#L7-L7) Diagram 2 (Backend AI/ML Service Architecture)

---

## Answer Generation Process

The Hiring Assistant Service uses a multi-stage process to generate contextual, role-specific interview answers.

### Generation Pipeline

### Prompt Context Structure

The service constructs a comprehensive prompt context that includes:

1. **Role Information**: Target position and responsibilities
2. **Questions List**: Interview questions to be answered
3. **Company Context**: Research findings about the company
4. **User Knowledge**: Candidate's existing understanding
5. **Resume Context**: Implicit from session/request (passed separately)
6. **Word Limit**: Constraint for each answer

### LLM Configuration

The service uses the shared Google Gemini LLM instance:

* **Model**: `gemini-2.0-flash` [backend/app/core/llm.py7](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L7-L7)
* **Temperature**: `0.1` (low variability for consistent answers) [backend/app/core/llm.py22](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L22-L22)
* **Provider**: Google Generative AI via LangChain [backend/app/core/llm.py1](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L1-L1)

**Sources:** [backend/app/core/llm.py1-37](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L1-L37) [backend/app/models/schemas.py204-232](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L232) Diagram 2 (Backend AI/ML Service Architecture)

---

## Answer Quality Features

The Hiring Assistant Service incorporates several features to ensure high-quality, relevant answers:

### Word Limit Enforcement

Answers are constrained by the `word_limit` parameter with validation:

* Minimum: 50 words
* Maximum: 500 words
* Default: 150 words

This ensures answers are concise yet comprehensive enough for interview preparation.

### Contextual Relevance

Answers incorporate multiple context layers:

| Context Type | Source | Purpose |
| --- | --- | --- |
| Role-specific | `role` field | Tailor answers to position requirements |
| Company-specific | `company_research_info` | Align with company culture/values |
| Candidate knowledge | `user_knowledge` field | Build on existing understanding |
| Resume data | Session context | Reference actual experience |

### Multi-Question Support

The service processes multiple questions in a single request, maintaining consistency across answers while providing question-specific responses. The response `data` field maps each question to its corresponding answer [backend/app/models/schemas.py238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L238-L238)

**Sources:** [backend/app/models/schemas.py227-232](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L227-L232) Diagram 4 (Data Flow for Resume Analysis)

---

## Integration Points

### Frontend Integration

The Hiring Assistant Service is accessed from the dashboard interface at `/dashboard/hiring-assistant`:

The frontend pre-populates resume context from localStorage, which is set during resume analysis [backend/app/models/schemas.py173](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L173-L173) This enables seamless transition from resume analysis to interview preparation.

### Backend Service Dependencies

The service relies on several backend components:

1. **LLM Core**: Shared Google Gemini instance [backend/app/core/llm.py6-28](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L6-L28)
2. **Web Content Agent**: Markdown extraction via Jina AI [backend/app/agents/\_\_init\_\_.py7](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py#L7-L7)
3. **Web Search Agent**: Company research via Tavily [backend/app/agents/\_\_init\_\_.py5](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py#L5-L5)
4. **Company Research Function**: `get_company_research()` from `app.services.hiring_assiatnat`

### Cross-Service Integration

The Hiring Assistant shares infrastructure with other AI services:

Both the Hiring Assistant and Cold Mail Generator use the same company research function [backend/app/services/cold\_mail.py7-311](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L7-L311) demonstrating shared service patterns across the backend.

**Sources:** [backend/app/services/cold\_mail.py7-311](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L7-L311) [backend/app/core/llm.py1-37](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L1-L37) [backend/app/agents/\_\_init\_\_.py1-13](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/agents/__init__.py#L1-L13) Diagram 3 (Frontend Application Architecture), Diagram 4 (Data Flow for Resume Analysis)

---

## Error Handling and Validation

The service implements comprehensive validation and error handling:

### Request Validation

Validation is enforced through Pydantic models [backend/app/models/schemas.py204-232](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L232):

* **Field Constraints**: Minimum lengths and range validation
* **Type Checking**: Strict type enforcement for all fields
* **Default Values**: Sensible defaults for optional parameters

### Common Error Scenarios

| Error Type | Status Code | Trigger Condition |
| --- | --- | --- |
| Validation Error | 400 | Missing required fields or invalid types |
| Service Unavailable | 503 | LLM service not initialized |
| Processing Error | 500 | Company research failure or LLM timeout |
| Invalid Word Limit | 400 | word\_limit outside 50-500 range |

### LLM Service Availability Check

Following the pattern from related services, the Hiring Assistant checks LLM availability:

```
```
# Pattern from cold_mail.py
if not llm:
    raise HTTPException(
        status_code=503,
        detail=ErrorResponse(
            message="LLM service is not available."
        ).model_dump(),
    )
```

This check occurs at the service entry point before processing the request [backend/app/services/cold\_mail.py255-258](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L255-L258)

**Sources:** [backend/app/models/schemas.py204-238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L238) [backend/app/services/cold\_mail.py255-258](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L255-L258) [backend/app/core/llm.py6-17](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L6-L17)

---

## Usage Workflow

The typical usage workflow for the Hiring Assistant Service:

**Sources:** [backend/app/models/schemas.py204-238](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/models/schemas.py#L204-L238) [backend/app/services/cold\_mail.py308-311](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L308-L311) Diagram 3 (Frontend Application Architecture), Diagram 4 (Data Flow for Resume Analysis)

---

## Performance Considerations

### Processing Time

The service processing time is influenced by:

1. **Company Research** (0-2 seconds): Optional, depends on `company_url` presence
2. **LLM Generation** (2-5 seconds per question): Varies with number of questions and word limit
3. **Total Latency**: Approximately 2-10 seconds for typical requests

### Optimization Strategies

The service can leverage several optimization techniques:

* **Batch Processing**: Multiple questions processed in single LLM call
* **Research Caching**: Company research results can be cached for repeated requests
* **Word Limit Tuning**: Lower word limits reduce LLM generation time
* **Faster Model**: Could use `gemini-2.0-flash-lite` [backend/app/core/llm.py10](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L10-L10) for improved speed with minimal quality tradeoff

### Concurrent Request Handling

The FastAPI backend supports asynchronous processing, enabling efficient handling of multiple concurrent interview preparation requests. The service follows async patterns used by other AI services [backend/app/services/cold\_mail.py445](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L445-L445)

**Sources:** [backend/app/core/llm.py7-28](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/core/llm.py#L7-L28) [backend/app/services/cold\_mail.py445](https://github.com/harleenkaur28/AI-Resume-Parser/blob/b2bbd83d/backend/app/services/cold_mail.py#L445-L445) Diagram 6 (Deployment & Infrastructure Architecture)